PREPROCESSING:


pip install PyPDF2
from google.colab import drive
drive.mount('/content/drive')
import os
import spacy
import PyPDF2 
from csv import writer
# This is my path
path = "/content/drive/MyDrive/Resume Dataset"
 
dir_list = os.listdir(path)
 
print("Files and directories in '", path, "' :")
 
# prints all files
print(dir_list)


n=len(dir_list)
nlp = spacy.load('en_core_web_sm')
def normalize(text):
  norm_text=""
  for token in text:
    if not token.is_punct and not token.is_space:
      norm_text= norm_text + token.lemma_.lower()
  #print(norm_text)
  return norm_text


i=0    
while(i<n):
  pdfFileObj = open("/content/drive/MyDrive/Resume Dataset/"+dir_list[i], 'rb') 
  pdfReader = PyPDF2.PdfFileReader(pdfFileObj) 
  #print(pdfReader.numPages) 
  number_of_pages= pdfReader.numPages
  text=""
  for page_number in range(0,number_of_pages):
    page = pdfReader.getPage(page_number)
    txt = page.extractText()  
    text=text+txt
  #print(text) 
  pdfFileObj.close()
  doc = nlp(text)
  normalized_text=normalize(doc)
  entry=[normalized_text]
  with open('dataset.csv', 'a') as f_object:
  
    # Pass this file object to csv.writer()
    # and get a writer object
    writer_object = writer(f_object)
  
    # Pass the list as an argument into
    # the writerow()
    writer_object.writerow(entry)
  
    #Close the file object
    f_object.close()
  i=i+1




CNN:


import numpy as np
import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences


from keras.layers import Input, Embedding, Activation, Flatten, Dense
from keras.layers import Conv1D, MaxPooling1D, Dropout
from keras.models import Model
data_source = pd.read_csv('/content/sample_data/Preprocessed Data.csv', header=None)




train_df = data_source[:500]
test_df = data_source[500:]
# convert string to lower case
train_texts = train_df[1].values
train_texts = [s.lower() for s in train_texts]


test_texts = test_df[1].values
test_texts = [s.lower() for s in test_texts]
tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')
tk.fit_on_texts(train_texts)


alphabet = "abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\"/\\|_@#$%^&*~`+-=<>()[]{}"
char_dict = {}
for i, char in enumerate(alphabet):
    char_dict[char] = i + 1




tk.word_index = char_dict.copy()


tk.word_index[tk.oov_token] = max(char_dict.values()) + 1






# Convert string to index
train_sequences = tk.texts_to_sequences(train_texts)
test_texts = tk.texts_to_sequences(test_texts)


# Padding
train_data = pad_sequences(train_sequences, maxlen=1014, padding='post')
test_data = pad_sequences(test_texts, maxlen=1014, padding='post')


# Convert to numpy array
train_data = np.array(train_data, dtype='float32')
test_data = np.array(test_data, dtype='float32')
train_classes = train_df[0].values
train_class_list = [x - 1 for x in train_classes]




test_classes = test_df[0].values
test_class_list = [x - 1 for x in test_classes]


!pip install keras
import keras
from tensorflow.keras.utils import to_categorical


train_classes = to_categorical(train_class_list)
test_classes = to_categorical(test_class_list)


train_classes.shape
  
input_size = 1014
vocab_size = len(tk.word_index)
embedding_size = 69
conv_layers = [[256, 7, 3],
               [256, 7, 3],
               [256, 3, -1],
               [256, 3, -1],
               [256, 3, -1],
               [256, 3, 3]]


fully_connected_layers = [1024, 1024]
num_of_classes = 4
dropout_p = 0.5
optimizer = 'adam'
loss = 'categorical_crossentropy'




embedding_weights = []  # (70, 69)
embedding_weights.append(np.zeros(vocab_size))  # (0, 69)


for char, i in tk.word_index.items():  # from index 1 to 69
    onehot = np.zeros(vocab_size)
    onehot[i - 1] = 1
    embedding_weights.append(onehot)


embedding_weights = np.array(embedding_weights)
print('Load')




embedding_layer = Embedding(vocab_size + 1,
                            embedding_size,
                            input_length=input_size,
                            weights=[embedding_weights])




inputs = Input(shape=(input_size,), name='input', dtype='int64')  # shape=(?, 1014)


x = embedding_layer(inputs)


for filter_num, filter_size, pooling_size in conv_layers:
    x = Conv1D(filter_num, filter_size)(x)
    x = Activation('relu')(x)
    if pooling_size != -1:
        x = MaxPooling1D(pool_size=pooling_size)(x)  # Final shape=(None, 34, 256)
x = Flatten()(x)  # (None, 8704)
# Fully connected layers
for dense_size in fully_connected_layers:
    x = Dense(dense_size, activation='relu')(x)  # dense_size == 1024
    x = Dropout(dropout_p)(x)




predictions = Dense(num_of_classes, activation='softmax')(x)




model = Model(inputs=inputs, outputs=predictions)
model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])  # Adam, categorical_crossentropy
model.summary()


# Shuffle
indices = np.arange(train_data.shape[0])
np.random.shuffle(indices)


x_train = train_data[indices][:1000]
y_train = train_classes[indices][:1000]


x_test = test_data[:100]
y_test = test_classes[:100]


# Training
model.fit(x_train, y_train,
          validation_data=(x_test, y_test),
          batch_size=128,
          epochs=10,
          verbose=2)
# save the model to file
model.save('model.h5')


import tensorflow as tf


model = tf.keras.models.load_model("model.h5")


 op=train_texts[0:1]
tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')
tk.fit_on_texts(op)
op_sequences = tk.texts_to_sequences(op)
op_data = pad_sequences(op_sequences, maxlen=1014, padding='post')
op_data = np.array(op_data, dtype='float32')


prediction = model.predict([op_data])
print(prediction)








#import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
#import os
#print(os.listdir("../input"))
import spacy
import random 
#from collections import Counter #for counting
#import seaborn as sns #for visualization
nlp = spacy.load("en_core_web_sm")
t = pd.read_csv("/content/sample_data/preprocessed.csv", header=None)
t.shape
t.head(4)
text_combined = str(t)
print(text_combined)
!pip install nltk
import nltk
import spacy
from csv import writer


nltk.download('punkt')
from nltk import word_tokenize
nltk.download('averaged_perceptron_tagger')
text = word_tokenize(text_combined)
nltk.pos_tag(text)


for token in doc:
  r1=[token.text, token.pos_,token.ent_iob_]
  print(r1)
  with open('event.csv', 'a') as f_object:
     writer_object = writer(f_object)
     writer_object.writerow(r1)


  f_object.close()


BI-LSTM:


import pandas as pd
data = pd.read_csv('/content/sample_data/ner_dataset.csv', encoding= 'unicode_escape')
data.head(n=24)
data=data[0:10000]
from itertools import chain
def get_dict_map(data, token_or_tag):
    tok2idx = {}
    idx2tok = {}
    
    if token_or_tag == 'token':
        vocab = list(set(data['Word'].to_list()))
    else:
        vocab = list(set(data['Tag'].to_list()))
    
    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}
    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}
    return tok2idx, idx2tok




token2idx, idx2token = get_dict_map(data, 'token')
tag2idx, idx2tag = get_dict_map(data, 'tag')
data['Word_idx'] = data['Word'].map(token2idx)
data['Tag_idx'] = data['Tag'].map(tag2idx)
data.head()
# Fill na
data_fillna = data.fillna(method='ffill', axis=0)
# Groupby and collect columns
data_group = data_fillna.groupby(
['Sentence #'],as_index=False
)['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))
# Visualise data
data_group.head()
from sklearn.model_selection import train_test_split
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
def get_pad_train_test_val(data_group, data):


    #get max token and tag length
    n_token = len(list(set(data['Word'].to_list())))
    n_tag = len(list(set(data['Tag'].to_list())))


    #Pad tokens (X var)    
    tokens = data_group['Word_idx'].tolist()
    maxlen = max([len(s) for s in tokens])
    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)


    #Pad Tags (y var) and convert it into one hot encoding
    tags = data_group['Tag_idx'].tolist()
    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx["O"])
    n_tags = len(tag2idx)
    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]


    
    #Split train, test and validation set
    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)
    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,test_size = 0.25,train_size =0.75, random_state=2020)


    print(
        'train_tokens length:', len(train_tokens),
        '\ntest_tokens length:', len(test_tokens),
        '\nval_tokens:', len(val_tokens),
        '\n\ntrain_tags:', len(train_tags),
        '\ntest_tags:', len(test_tags),
        '\nval_tags:', len(val_tags),


    )
    
    return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags


train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(data_group, data)
import numpy as np
import tensorflow
from tensorflow.keras import Sequential, Model, Input
from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional
from tensorflow.keras.utils import plot_model
from numpy.random import seed
seed(1)
tensorflow.random.set_seed(2)
input_dim = len(list(set(data['Word'].to_list())))+1
output_dim = 64
input_length = max([len(s) for s in data_group['Word_idx'].tolist()])
n_tags = len(tag2idx)
print('input_dim: ', input_dim, '\noutput_dim: ', output_dim, '\ninput_length: ', input_length, '\nn_tags: ', n_tags)
def get_bilstm_lstm_model():
    model = Sequential()


    # Add Embedding layer
    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))


    # Add bidirectional LSTM
    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))


    # Add LSTM
    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))


    # Add timeDistributed Layer
    model.add(TimeDistributed(Dense(n_tags, activation="relu")))




    # Compile model
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.summary()
    
    return model
def train_model(X, y, model):
    loss = list()
    for i in range(25):
        # fit model for one epoch on this sequence
        hist = model.fit(X, y, batch_size=1000, verbose=1, epochs=1, validation_split=0.2)
        loss.append(hist.history['loss'][0])
    return loss


results = pd.DataFrame()
model_bilstm_lstm = get_bilstm_lstm_model()
plot_model(model_bilstm_lstm)
results['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)
model_bilstm_lstm.save('model.hi')
dat = pd.read_csv('/content/sample_data/dataaaa - Sheet1.csv', encoding= 'unicode_escape')
token2idx, idx2token = get_dict_map(data, 'token')
dat['Word_idx'] = data['Word'].map(token2idx)
dat_fillna = dat.fillna(method='ffill', axis=0)


dat_group = dat_fillna.groupby(
['Sentence #'],as_index=False
)['Word', 'POS', 'Tag', 'Word_idx'].agg(lambda x: list(x))
# Visualise data
dat_group.head()
n_token = len(list(set(dat['Word'].to_list())))
tokens = data_group['Word_idx'].tolist()
maxlen = max([len(s) for s in tokens])
op = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)
import tensorflow as tf
model = tf.keras.models.load_model("model.hi")


prediction = model.predict([op[:1]])
print(prediction)






RECOMMENDER:


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
df = pd.read_csv('/content/sample_data/Placement_Data_Full_Class.csv',
                 lineterminator='\n')
df.isnull().sum()
df1 = df.drop(columns = ['ssc_b'])
from sklearn.feature_extraction.text import TfidfVectorizer
tf = TfidfVectorizer()
tfidf_matrix_spec = tf.fit_transform(df1['specialisation'])
tfidf_matrix_deg = tf.fit_transform(df1['degree_t'])
a = pd.DataFrame(tfidf_matrix_deg.todense())
from sklearn.metrics.pairwise import cosine_similarity
cosine_sim_spec = cosine_similarity(tfidf_matrix_spec, tfidf_matrix_spec)
cosine_sim_deg = cosine_similarity(tfidf_matrix_deg, tfidf_matrix_deg)


names = df1['name']
indices = pd.Series(df.index, index=df1['name'])
print(indices.head(10))
sim_scores_spec = list(enumerate(cosine_sim_spec[indices['Rahul']]))
sim_scores_deg = list(enumerate(cosine_sim_deg[indices['Rahul']]))
sim_scores=[]
for x in sim_scores_spec:
  for y in sim_scores_deg:
    if(x[0]==y[0]):
      
      sim_scores.append((x[0],x[1]*y[1]))
      


sim=sorted(sim_scores,key=lambda a: a[1],reverse=True)
sim=sim[0:10]
indice= [i[0] for i in sim]
names.iloc[indice]













